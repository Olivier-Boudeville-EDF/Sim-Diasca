:raw-latex:`\pagebreak`

-------------------------------------------
Sim-Diasca Management of Probabilistic Laws
-------------------------------------------


Principles
==========

Quite often models rely on the use of stochastic variables.

Such variables - respecting a given probabilistic law - are very useful to model the behaviours of simulation actors.

For example, in terms of reliability, the approach is generally to define per-equipment average constants like the *Mean Time To Failure* (MTTF) and the *Mean Time To Repair* [#]_ (MTTR), and to express reliability models thanks to stochastic laws parameterised by these constants (in that case: exponential and Gaussian laws, respectively).


.. [#] MTTR is also known as *Mean Time To Recovery*.


Stochastic variables are also very useful when *generating* variations on a theme, in the context of the simulation of a large number of instances of a given case.

For example, when wanting to simulate a wide range of different low-voltage meshes, one can either:

 - load a set of externally-defined descriptions of real meshes coming from the field (should such descriptions be available, accurate and numerous enough)

 - or, more easily, one can *generate* these different meshes according to some probabilistic rules, which would give, for example, the number of supply points of a given mesh, depending on its profile (ex: rural, urban, etc.), so that the pool of generated meshes follows the real-life statistics


The objective here is therefore to provide model developers with all the facilities needed to easily specify and implement stochastic models, in full compliance with the aforementioned simulation properties. This involves a little more than that:

:raw-html:`<center><img src="xkcd-random_number.png"></img></center>`
:raw-latex:`\includegraphics[scale=0.7]{xkcd-random_number.png}`




Built-in Random Distributions
=============================

Although any probabilistic distribution (i.e. probability density) can be defined and added to the framework, Sim-Diasca provides some built-in distributions, listed below, that are among the most common.

They are proposed by the ``RandomManager``, a specific technical component helping to manage stochastic variables in the context of a distributed simulation, notably so that causality and reproducibility are preserved.


Uniform Law
-----------

This "white noise" generator will draw values into a user-parameterised range, all samples having an equal probability of being chosen.

When placing a Sim-Diasca `probe`_ at the output of a RandomManager set to deliver a uniform law, we have the following result:

:raw-html:`<center><img src="RandomManager-Uniform_probe.png"></img></center>`
:raw-latex:`\includegraphics[scale=0.6]{RandomManager-Uniform_probe.png}`

By setting the appropriate flag, Sim-Diasca can also be configured to use a uniform generator of superior quality, which was designed for cryptography.

This uniform distribution is often the basis to generate in turn more complex distributions.


Exponential Law
---------------

This distribution, defined by a single parameter, ``lambda``, leads to the following result:

:raw-html:`<center><img src="RandomManager-Exponential_probe.png"></img></center>`
:raw-latex:`\includegraphics[scale=0.6]{RandomManager-Exponential_probe.png}`

This distribution is directly generated by Sim-Diasca from the previous white noise source.



Gaussian Law
------------

Two parameters, ``mu``, the mean value, et ``sigma``, the variance (whose positive square root is the standard deviation), define the Gaussian law [#]_.

.. [#] It is also known as the normal law.

This results in the following curve:

:raw-html:`<center><img src="RandomManager-Gaussian_probe.png"></img></center>`
:raw-latex:`\includegraphics[scale=0.6]{RandomManager-Gaussian_probe.png}`

The Sim-Diasca white noise generator is used to generate this Gaussian law as well.



Actual Management of Randomness
===============================


Random Generators
-----------------

At the core of most implementations, one relies on a random generator, which usually outputs floating-point values uniformly distributed between 0.0 and 1.0.

For a better stochastic management, the engine does not rely anymore on the basic ``random`` module of Erlang; it may operate instead with the ``crypto`` module (if available and enabled), otherwise it will default on the newer ``rand`` module, which offers various algorithms, including:

 - ``exsplus``: Xorshift116+, 58 bits precision and period of 2^116-1 (state uses 320 bytes on 64-bit platforms)
 - ``exs64``: Xorshift64*, 64 bits precision and a period of 2^64-1 (state of 336 bytes on 64-bit platforms)
 - ``exs1024``: Xorshift1024*, 64 bits precision and a period of 2^1024-1 (state of 856 bytes on 64-bit platforms)

Unless overridden (see ``random_utils.erl``), the algorithm used by the engine is ``exsplus``.

Based on a uniform random value in ``[0.0,1.0]``, one can generate uniform values in any other range, and values that respect all kinds of non-uniform laws (like the Gaussian one).

However a question still remains: how many instances of random generators should we use?


Mode Of Operation
-----------------

Random generators usually have a state, which is initialised with a seed - either set by default, or specifically given.

From a seed a series of random numbers can be generated, and as such it can be reproduced identically, as long as the same seed is used.

The trouble comes from the fact that, during any given logical moment (diasca), multiple simulation actors may require - and therefore request from a random generator - any number of values complying to any number of probabilistic laws, each parameterised as wished, in any order. And of course we do not want to loose the reproducibility of simulations because of that.


Initially the engine was relying on a limited number of centralised random manager instances (possibly even just one), each used by a (potentially large) set of model instances.

Each of these model instances would then interact with its random manager(s) in a consistent manner, through actor messages to preserve simulation properties.

Such an approach induces many constraints, like the additional synchronisation and diasca creation involved (hence significant runtime overhead), a more complex model-level logic to request and wait for these values, etc.



Detailed: Why Centralised Random Managers are Evil
..................................................


(please feel free to skip this section if not having historical curiosity)

The most obvious approach for stochastic management is to have actors require the random values they need to a centralised random manager.

This solution is simple, but has some pitfalls, particularly if the engine does not provide a concept of "logical moments", i.e. diascas here.

A central objective is of course **not to break reproducibility**. Indeed, without any specific measure, actors would request their value to the centralised random manager during the execution of their tick, with no particular order enforced between requests, since they would be concurrent in that context.

Therefore, if, thanks to the seeding, they would indeed consume collectively always the same random series, the values of this series would be differently dispatched among actors, depending on the chronological order of reception of their requests by the random manager.

A solution is to **have the random manager become a simulator actor** as well. Then it would be appropriately synchronised by the mechanisms provided by the time manager, and stochastic actors would thus behave correctly and in a reproducible way.

There is an issue there nevertheless. Indeed, if the model of an actor required that actor to use a random value at a given tick N, then to have that value the actor would have to send a request during this tick to the random manager, which would process that request during the next tick (i.e. N+1) and send back the determined value to the requesting actor, which would in turn be able to process it no sooner than the next tick (N+2).

Therefore this would induce by default an **unwanted 2-tick latency** each time an actor would require a random value, whereas the model would not tell us so. As some actors can consume at least one value per tick, the system cannot work as is.

Moreover, not all actors are able to anticipate on their needs of random values, and, in the cases where it would be possible, doing so would make their implementation a lot more complex than needed.

Hence, before diascas were introduced, a generic solution had been designed instead - which would manage transparently these needs, i.e. with no impact on the writing of models.

The solution consists on having each actor that uses stochastic variables define, for each one of them, not only which distribution law with which settings should be used, but also an **upper bound to the number of values following that law that may be drawn during any single tick**, for this actor and this distribution.

Such an upper bound should be possible to define for most if not all models and, if ever the upper bound was incorrectly evaluated (i.e. if it was set to a too small value, leading to an exhaustion of the corresponding random buffer), then at simulation time the issue would be detected and the simulation would stop. Then the upper-bound could just be set to a higher value, and the simulation be run again.


With these information, the generic *stochastic actor* (a Sim-Diasca built-in) was able to transparently cache full lists of random variables obtained from the (centralised) random manager, and to manage their refilling appropriately in the background, so that the corresponding random values could be always obtained with zero latency by an actor.

Thus **the implementation of models was considerably simplified**, since they can be developed as if they could rely on local infinite random sources, which additionally would not raise issues about reproducibility.

This was coming at the expense of extra diascas being instantiated (forcing models to manage them and hitting the runtime performances) and a few extra constraints.

For example, apart from the already mentioned constraint regarding the upper bound in terms of the number of drawn samples, some stochastic actors need random variables whose probabilistic distributions can change during a simulation. For example, if a meter determines its connectivity by drawing, even with equal probability, a given number of meters out of its functional upstream meters, this translates into a uniform law whose range can change at each tick (depending on how many upstream meters are functional); this is a problem for this kind of approaches based on transparent buffering.

The specialised generic actor, the ``Stochastic Actor`` - which can be reused at will by all stochastic models to simplify their development - used to rely on this mechanism. Since then, we opted for a simpler and more efficient system, explained below.



Current Mode of Operation
.........................


A more flexible approach has been finally retained: each model instance embeds its **own, private, random generator** (to which it can readily and freely access without constraint), which is seeded appropriately (on a reproducible manner, each actor having its own, specific seed) when that actor is created.

This removes all drawbacks previously mentioned, at the expense of:

 - a more complex actor creation done by the load balancer (in charge of a parallel yet proper seeding - itself based on its own random generator)
 - an increased memory footprint of the stochastic actors, as each must store the state of its random generator (typically ranging from 32 bytes to a few kilobytes)


Thanks to this on-creation seeding, reproducibility is ensured, and stochastic actors are able to interact with their embedded random generator with no further synchronisation effort, i.e. with no delay nor message (actor or not).


Randomness Pitfalls
===================

All model instances are automatically correctly seeded, so all probabilistic laws can be readily used from them with no effort.

However, in some cases (typically for initialisation purposes, in the simulation case) it may be useful to rely on basic processes, WOOPER or not (i.e. not actors), and some of them might have to be stochastic (ex: when generating a given road network following specific constraints). **These helper processes should have their random source explicitly seeded** (using ``random_utils:start_random_source/{1,3}`` for that), otherwise a non-constant seed will be assigned to each of them, and it will break reproducibility.

Another potential cause of issue is the change of a random source: if not explicitly seeded, some of them will default on a constant seed (ex: ``random``) while others not (ex: ``rand``, the current default source).

As a result, **all non-actor processes having to generate, directly or not, random values shall be explicitly seeded**, typically thanks to::

  random_utils:start_random_source( default_seed )
